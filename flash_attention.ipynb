{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online normalizer calculation for softmax\n",
    "\n",
    "This is a CPU implementation of the [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input vector\n",
    "x = np.random.randn(100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_softmax(x):\n",
    "    total = 0\n",
    "    # first load each element to get the sum of the exponential\n",
    "    for v in x:\n",
    "        total += math.exp(v)\n",
    "    # second load each element to get the result\n",
    "    res = []\n",
    "    # one store operation\n",
    "    for v in x:\n",
    "        res.append(math.exp(v) / total)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_softmax_res = standard_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_softmax(x):\n",
    "    max_value = float('-inf')\n",
    "    # first load each element to find the max value\n",
    "    for v in x:\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "    total = 0.0\n",
    "    # second load each element to calculate the sum of the exponential\n",
    "    for v in x:\n",
    "        total += math.exp(v - max_value)\n",
    "    # third load each element to calculate the result\n",
    "    res = []\n",
    "    # one store operation\n",
    "    for v in x:\n",
    "        res.append(math.exp(v - max_value) / total)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_softmax_res = safe_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# safe and standard softmax should be the same\n",
    "np.allclose(standard_softmax_res, safe_softmax_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax(x):\n",
    "    total = 0.0\n",
    "    max_value = float('-inf')\n",
    "    # first load each element to find the max value and sum of the exponential\n",
    "    for v in x:\n",
    "        old_max = max_value\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "        total = total * math.exp(old_max - max_value) + math.exp(v - max_value)\n",
    "    # second load each element to calculate the result\n",
    "    res = []\n",
    "    # one store operation\n",
    "    for v in x:\n",
    "        res.append(math.exp(v - max_value) / total)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_softmax_res = online_softmax(x)\n",
    "np.allclose(online_softmax_res, safe_softmax_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention\n",
    "\n",
    "Flash attention paper: [Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)\n",
    "\n",
    "Flash attention is building on top of the online normalizer calculation for softmax. So a good understanding of the online normalizer calculation for softmax will help us understand flash attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, d = 9, 9\n",
    "q = np.random.randn(N, d)\n",
    "k = np.random.randn(N, d)\n",
    "v = np.random.randn(N, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the block size for row and column\n",
    "b_r, b_c = 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention(q, k, v, b_r, b_c):\n",
    "    \"\"\"\n",
    "    Calculates the attention with flash attention method.\n",
    "    \n",
    "    The FLOPs is O(N^2 * d)\n",
    "    \n",
    "    The additional memory required is for m and l which is O(N)\n",
    "\n",
    "    Args:\n",
    "        q: Query matrix (shape: (N, d))\n",
    "        k: Key matrix (shape: (N, d))\n",
    "        v: Value matrix (shape: (N, d))\n",
    "        b_r: The block size for q\n",
    "        b_c: The block size for k and v\n",
    "    Returns:\n",
    "        attention_output: Output of the attention mechanism (shape: (N, d))\n",
    "    \"\"\"\n",
    "    assert q.shape[0] % b_r == 0, \"the number of rows of q must be divisible by b_r\"\n",
    "    assert k.shape[0] % b_c == 0, \"the number of rows of k must be divisible by b_c\"\n",
    "    assert v.shape[0] % b_c == 0, \"the number of rows of v must be divisible by b_c\"\n",
    "    assert q.shape[1] == k.shape[1] == v.shape[1], \"the number of columns of q, k, v must be the same\"\n",
    "    \n",
    "    # the output of the flash attention\n",
    "    o = np.zeros_like(q)\n",
    "\n",
    "    N = q.shape[0]\n",
    "    # the sum of exponential vector for each row\n",
    "    l = np.zeros((N, 1))\n",
    "    # the max value for each row\n",
    "    m = np.full((N, 1), float('-inf'))\n",
    "    \n",
    "    # divide the q into blocks and each block has size b_r * d\n",
    "    q_blocks = [q[i:i+b_r, :] for i in range(0, N, b_r)]\n",
    "    # divide the k into blocks and each block has size b_c * d\n",
    "    k_blocks = [k[i:i+b_c, :] for i in range(0, N, b_c)]\n",
    "    # divide the v into blocks and each block has size b_c * d\n",
    "    v_blocks = [v[i:i+b_c, :] for i in range(0, N, b_c)]\n",
    "    # divide the o into blocks and each block has size b_r * d\n",
    "    o_blocks = [o[i:i+b_r, :] for i in range(0, N, b_r)]\n",
    "    # divide the l into blocks and each block has size b_r * 1\n",
    "    l_blocks = [l[i:i+b_r, :] for i in range(0, N, b_r)]\n",
    "    # divide the m into blocks and each block has size b_r * 1\n",
    "    m_blocks = [m[i:i+b_r, :] for i in range(0, N, b_r)]\n",
    "    \n",
    "    n_q_blocks = len(q_blocks)\n",
    "    n_k_blocks = len(k_blocks)\n",
    "    \n",
    "    for j in range(n_k_blocks):\n",
    "        # load k_j and v_j from HBM to on-chip SRAM, line 6\n",
    "        k_block = k_blocks[j]\n",
    "        v_block = v_blocks[j]\n",
    "        # so for FLOPs, the dominant part is b_r * b_c * d, we have n_k_blocks * n_q_blocks * b_r * b_c * d which is O(N^2 * d)\n",
    "        for i in range(n_q_blocks):\n",
    "            # load q_i, m_i, l_i, o_i from HBM to on-chip SRAM, line 8\n",
    "            q_block, m_block, l_block, o_block = q_blocks[i], m_blocks[i], l_blocks[i], o_blocks[i]\n",
    "            # calculate the dot product of size b_r * b_c, line 9, FLOPs: b_r * b_c * d\n",
    "            s_i_j = np.matmul(q_block, k_block.T)\n",
    "            # calculate the max value for each row, b_r * 1, line 10, FLOPs: b_r * d\n",
    "            m_i_j = np.max(s_i_j, axis=1, keepdims=True)\n",
    "            # calculate nominator of the softmax of size b_r * b_c, line 10, FLOPs: b_r * b_c * d\n",
    "            p_i_j = np.exp(s_i_j - m_i_j)\n",
    "            # calcualte the sum of the exponential for each row, line 10, FLOPs: b_r * d\n",
    "            l_i_j = np.sum(p_i_j, axis=1, keepdims=True)\n",
    "            # get the new max value for each row, line 11, FLOPs: b_r * d\n",
    "            m_i_new = np.maximum(m_block, m_i_j)\n",
    "            # get the new sum of exponential vector for each row, line 11, FLOPs: 2 * (b_r * d + d)\n",
    "            l_i_new = l_block * np.exp(m_block - m_i_new) + l_i_j * np.exp(m_i_j - m_i_new)\n",
    "            # update the output matrix O, line 12, FLOPs: (b_r * b_c * d + b_r * d) + b_r * d + b_r\n",
    "            current_o_block = np.exp(m_i_j - m_i_new) * p_i_j @ v_block # b_r * d\n",
    "            updated_old_o_block = l_block * np.exp(m_block - m_i_new) * o_blocks[i] # FLOPs: b_r * d + b_r\n",
    "            # update output matrix block and store to HBM, line 12, FLOPs: 2 * (b_r * d + b_r)\n",
    "            o_blocks[i] = (current_o_block + updated_old_o_block) / l_i_new # b_r * d\n",
    "            # update the max value for each row and store to HBM, line 13, FLOPs: b_r\n",
    "            m_blocks[i] = m_i_new # b_r * 1\n",
    "            # update the sum of exponential vector for each row and store to HBM, line 13, FLOPs: b_r\n",
    "            l_blocks[i] = l_i_new # b_r * 1\n",
    "            \n",
    "    o = np.concatenate(o_blocks, axis=0)\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash_attention_res = flash_attention(q, k, v, b_r, b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify the flash attention is doing the right thing\n",
    "def standard_attention(Q, K, V):\n",
    "  \"\"\"\n",
    "  Calculates dot-product attention.\n",
    "  \n",
    "  The FLOPs is O(N * d^2)\n",
    "    \n",
    "  No additional memory required\n",
    "\n",
    "  Args:\n",
    "    Q: Query matrix (shape: (N, d))\n",
    "    K: Key matrix (shape: (N, d))\n",
    "    V: Value matrix (shape: (N, d))\n",
    "\n",
    "  Returns:\n",
    "    attention_output: Output of the attention mechanism (shape: (N, d))\n",
    "  \"\"\"\n",
    "  # For standard attention, the FLOPs is O(N * d^2) which should be smaller than flash attention since N is usually much larger than d\n",
    "  # Calculate attention scores\n",
    "  scores = np.matmul(Q, K.T)  # (sequence_length, sequence_length)\n",
    "\n",
    "  # Calculate attention weights\n",
    "  attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # (sequence_length, sequence_length)\n",
    "  attention_weights /= attention_weights.sum(axis=-1, keepdims=True) # (sequence_length, sequence_length)\n",
    "  # Calculate output of the attention\n",
    "  attention_output = np.matmul(attention_weights, V)  # (sequence_length, head_dim)\n",
    "  return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_attention_res = standard_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(flash_attention_res, standard_attention_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
